<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
		<title>Picking AI Models for SQL-based Data Analysis With Zaturn</title>
		<meta name="viewport" content="width=device-width, height=device-height, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover">
		<meta name="description" content="Zaturn lets you analyze your data using AI chat">
		<link rel="shortcut icon" type="image/png" href="/assets/logo.ico">
				
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:title" content="Picking AI Models for SQL-based Data Analysis With Zaturn">
		<meta name="twitter:description" content="Zaturn lets you analyze your data using AI chat">
		<meta name="twitter:url" content="https://zaturn.pro/ai-models-for-sql-data-analysis">
		<meta name="twitter:image" content="https://zaturn.pro/assets/covers/picking-models.webp">
	
		<meta property="og:site_name" content="kdqed">
		<meta property="og:type" content="article">
		<meta property="og:url" content="https://zaturn.pro/ai-models-for-sql-data-analysis">
		<meta property="og:title" content="Picking AI Models for SQL-based Data Analysis With Zaturn">
		<meta property="og:description" content="Zaturn lets you analyze your data using AI chat">

		<meta property="og:image" content="https://zaturn.pro/assets/covers/picking-models.webp">
	
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&family=Jost:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
	
	<style>
	  * {
	  	box-sizing: border-box;
	  	font-family: "Jost", sans-serif;
	  	margin: 0;
	  	padding: 0;
	  }

	  :root {
	    --bg1: #091018;
	    --bgt: #091018a0;
	    --bgt1: #091018a0;
	    --bgt2: #091018fa;
	    --bg2: #182030;
	    --fg1: #ffffff;
	    --fg2: #bad3e8;
	    --td: #175a92;
	    --tl: #5ca8e6;
	    --base-size: clamp(18px, 1.7vw, 32px);
	    --max-width: calc(var(--base-size)*42);
	    font-size: var(--base-size);
	    letter-spacing: 0.01rem;
	  }

	  body {
	    font-size: var(--base-size);
	    background: var(--bg1);
	    color: var(--fg2);
	    margin: 0;
	    padding: 0;
	  }
			
	  a {
	    color: inherit;
	  }

	  #backdrop {
	    position: fixed;
	    top: 0;
	    left: 0;
	    width: 100%;
	    height: 100vh;
	    background: url(/assets/covers/picking-models.webp);
	    background-position: left top;
	    background-size: cover;
	    z-index: -20;
	  }

	  header {
	    color: var(--fg1);
	    display: grid;
	    padding: 0.5rem;
	    align-items: center;
	    grid-template-columns: 1fr auto auto auto auto auto;
	    gap: 1rem;
	    background: var(--bgt1);
	    background-blend-mode: darken;
	    backdrop-filter: blur(48px);
	    position: fixed;
	    top: 0;
	    left: 0;
	    width: 100%;
	    height: auto;
	    z-index: 10;
	  }
      
      header>a.logo {
        display: inline-block;
        height: 2rem;
      }
      
			
	  header a {
	    font-size: 0.75rem;
	    text-decoration: none;
	    display: inline-flex;
	    align-content: center;
	  }

	  header>span {
		display: inline-flex;
	    align-content: center;
	  }

	  header a svg {
		width: 1rem;
		height: 1rem;
	  }

	  header a svg path {
		stroke: var(--fg1);
	  }
	  
	  header a.logo img {
		width: 2rem;
		height: 2rem;
	  } 
			
	  header .cta {
	    display: inline-flex;
	    align-items: center;
	    gap: 0.5rem;
	    background: var(--td);
	    font-size: 0.75rem;
	    text-decoration: none;
	    padding: 0.25rem 0.75rem;
	    border-radius: 0.25rem;
	    border: 0.1rem solid var(--td);
	  }
			
	  main {
	    text-align: center;
	    padding: 0;
	  }	

	  .hero-container, .content-container {
	    width: 100%;
	    text-align: center;
	  }

	  .hero-container {
	    color: var(--fg1);
	    display: grid;
	    place-items: center;
	    padding: 17vh 1rem 10vh 1rem;
	    background: var(--bgt1);
	    background-blend-mode: multiply;
	    backdrop-filter: blur(2px);
	  }

	  .content-container {
	    background: var(--bgt2);
	    padding-top: 5vh;
	    padding-bottom: 10vh;
	    background-blend-mode: multiply;
	    backdrop-filter: blur(64px);
	  }
			
	  section.hero {
	    display: inline-block;
	    width: 100%;
	    max-width: var(--max-width);
	  }
			
	  section.hero .dp {
	    width: 6rem;
	    height: auto;
	    border-radius: 100%;
	    border: 0.25rem solid var(--bgt);
	    margin-bottom: 1rem;
	  }

	  section.hero h2 {
		font-weight: 400;
		font-size: 1.25rem;
		color: var(--fg2);
	  }
			
	  section.content {
	    display: inline-block;
	    width: 100%;
	    max-width: var(--max-width);
	    text-align: left;
	    padding: 1rem;
	    color: var(--fg2);
	    line-height: 170%;
	  }
			
	  section.content>* {
	    margin-top: 1rem;
	  }

	  .yt-demo {
		width: 100%;
		aspect-ratio: 16 / 9;
	  }
		
	  footer {
	  	text-align: center;
	  	padding: 2rem 1rem 100vh 1rem;
	  }
			
	  footer>div {
	    background-blend-mode: multiply;
	    background: var(--bgt1);
	    color: var(--fg2);
	    font-size: 1rem;
	    backdrop-filter: blur(12px);
	    display: inline-flex;
	    flex-wrap: wrap;
	    align-items: center;
	    text-align: center;
	    justify-content: center;
	    gap: 1rem;
	    padding: 1rem;
	    border-radius: 0.5rem;
	  }
			
	  footer nav {
	    display: flex;
	    align-items: center;
	    text-align: center;
	    gap: 0.5rem;
	  }
			
	  footer nav a {
	    display: inline-flex;
	    height: 100%;
	    align-items: center;
	    color: inherit;
	  }
      
      footer nav svg {
        height: 1.25rem;
        width: auto;
      }
      
      footer nav svg path {
        stroke: var(--fg2);
        stroke-width: 0.07rem;
      }
      
	  h1 {
	    font-size: 1.6rem;
	    display: inline-block;
	    padding: 0.25rem 0.5rem;
	    border-radius: 0.25rem;
	    color: var(--fg1);
	  }
      
      section.content a {
        color: var(--tcyan);
      }
      
	  section.content>h2 {
	    color: var(--fg3);
	    font-size: 1.25rem;
	    color: var(--fg1);
	  }

	  section.content>h2:not(:first-child) {
	    margin-top: 3rem;
	  }
			
	  section.content>p>img {
	    width: 100%;
	  }
			
	  section.content>table {
	    width: 100%;
	    border-collapse: collapse;
	    display: block;
        overflow-x: auto;
        white-space: nowrap;
	  }
			
	  section.content>table th {
	    background: var(--fgt);
	    color: var(--bg1);
	    padding: 0.25rem 0.5rem;
	    font-size: 0.8rem;
	  }
			
	  section.content>table td {
	    background: var(--bgt2);
	    padding: 0.25rem 0.5rem;
	    font-size: 0.8rem;
	    border: 1px solid var(--fgt);
	    max-width: 60vw;
	    white-space: wrap;
	  }
			
	  section.content>table tr:nth-child(2n) td {
	    background: var(--bgt);
	  }

	  ul, ol, p {
	    font-size: inherit;
	  }
    
      ul li {
        margin-left: 0.5rem;
        padding-left: 0.5rem;
        margin-top: 0.25rem;
      }
      
      ol li {
        margin-left: 1.25rem;
        padding-left: 0.5rem;
        margin-top: 0.25rem;
      }
      
	  ul li::marker {
	    content: "o";
	  }
			
	  li>a:first-child {
	    color: var(--tcyan);
	  }
      
      blockquote {
        background: var(--bg1);
        padding: 0.5rem;
        font-size: 0.9rem;
        line-height: 160%;
        border-radius: 0.25rem;
        border: 0.15rem solid var(--tred);
      }
      
      blockquote>p {
        margin: 0;
      }

	  p>em:only-child {
		color: var(--tyellow);
	  }
      
      .text-badge {
        background: var(--tyellow);
        color: var(--fg3);
        display: inline-block;
        padding: 0.125rem 0.25rem;
        padding-bottom: 0.02rem;
        border-radius: 0.0rem;
        font-size: 75%;
      }

      pre {
		padding: 0.5rem;
		background: var(--bg2);
		border-radius: 0.1rem;
		overflow-x: auto;
		width: 100%;
		font-size: 80%;
      }

      pre>code, pre>code * {
		font-family: "Fira Code", monospace;
      }
		</style>
		
		
		<script type="application/ld+json">
		{
		  "@context": "https://schema.org",
		  "@type": "BlogPosting",
		  "headline": "Picking AI Models for SQL-based Data Analysis With Zaturn",
		  "image": [
		  	"/assets/covers/picking-models.webp"
		  ],
		  "datePublished": "2025-08-04 00:00:00 +0530",
		  "dateModified": "2025-08-04 00:00:00 +0530",
		  "author": [{
		  	"@type": "Person",
		  	"name": "Karthik Devan",
		  	"url": "https://kdqed.com"
		  }]
		}
		</script>
		
		
  </head>
  <body>
    <div id="backdrop"></div>
    <header>
	  <a href="/" class="logo">
        <img src="/assets/logo.png" alt="logo">
	  </a>
	  <a href="/docs">
        Docs
	  </a>
	  <a href="/blog">
        Blog
	  </a>
	  <a href="https://github.com/kdqed/zaturn">
		<svg stroke-width="1" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
		  <path d="M12 22C17.5228 22 22 17.5228 22 12C22 6.47715 17.5228 2 12 2C6.47715 2 2 6.47715 2 12C2 17.5228 6.47715 22 12 22Z" stroke="#000000" stroke-linecap="round" stroke-linejoin="round"></path>
		  <path d="M14.3333 19V17.137C14.3583 16.8275 14.3154 16.5163 14.2073 16.2242C14.0993 15.9321 13.9286 15.6657 13.7067 15.4428C15.8 15.2156 18 14.4431 18 10.8989C17.9998 9.99256 17.6418 9.12101 17 8.46461C17.3039 7.67171 17.2824 6.79528 16.94 6.01739C16.94 6.01739 16.1533 5.7902 14.3333 6.97811C12.8053 6.57488 11.1947 6.57488 9.66666 6.97811C7.84666 5.7902 7.05999 6.01739 7.05999 6.01739C6.71757 6.79528 6.69609 7.67171 6.99999 8.46461C6.35341 9.12588 5.99501 10.0053 5.99999 10.9183C5.99999 14.4366 8.19999 15.2091 10.2933 15.4622C10.074 15.6829 9.90483 15.9461 9.79686 16.2347C9.68889 16.5232 9.64453 16.8306 9.66666 17.137V19" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"></path>
		  <path d="M9.66667 17.7018C7.66667 18.3335 6 17.7018 5 15.7544" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"></path>
		</svg>
	  </a>
	  <a href="https://discord.gg/K8mECeVzpQ">
		<svg viewBox="0 0 24 24" stroke-width="1.5" fill="none" xmlns="http://www.w3.org/2000/svg" color="#000000">
		  <path d="M5.5 16C10.5 18.5 13.5 18.5 18.5 16" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"></path>
		  <path d="M15.5 17.5L16.5 19.5C16.5 19.5 20.6713 18.1717 22 16C22 15 22.5301 7.85339 19 5.5C17.5 4.5 15 4 15 4L14 6H12" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"></path>
		  <path d="M8.52832 17.5L7.52832 19.5C7.52832 19.5 3.35699 18.1717 2.02832 16C2.02832 15 1.49823 7.85339 5.02832 5.5C6.52832 4.5 9.02832 4 9.02832 4L10.0283 6H12.0283" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"></path>
		  <path d="M8.5 14C7.67157 14 7 13.1046 7 12C7 10.8954 7.67157 10 8.5 10C9.32843 10 10 10.8954 10 12C10 13.1046 9.32843 14 8.5 14Z" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"></path>
		  <path d="M15.5 14C14.6716 14 14 13.1046 14 12C14 10.8954 14.6716 10 15.5 10C16.3284 10 17 10.8954 17 12C17 13.1046 16.3284 14 15.5 14Z" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"></path>
		</svg>
	  </a>
	  <a href="/docs/install" class='cta'>
	    <span>Install</span>
	  </a>
    </header>
    <main>
	  <div class="hero-container">
		<section class="hero">
		  
		  <h1>Picking AI Models for SQL-based Data Analysis With Zaturn</h1>
		  
		</section>
	  </div>
	  <div class="content-container">
		<section class="content">
		  
				<p><em>August 4, 2025</em> by <a href="https://kdqed.com">Karthik Devan</a></p>
      
		  <p>There are many ways to use Zaturn to build an AI for data analysis. You might be using the Claude Desktop App with Zaturn as an MCP, or using an MCP client or AI agent framework where you bring your own LLM API. If you are connecting your own LLM API, you might be running the LLM locally with tools such as <a href="https://ollama.com">Ollama</a>, using the LLM developer’s official API, or using a 3rd party LLM API aggregator such as <a href="https://openrouter.ai/">OpenRouter</a> or <a href="https://www.litellm.ai/">LiteLLM</a>. In every case, there is an important decision to be made: which LLM do you use? In this blog, let’s break down the factors involved in picking an LLM for a data analysis agent.</p>

<h2 id="cost">Cost</h2>

<p>Quite obviously, the cost is the most important factor that can make or break a use case for an AI application. The cost itself depends on various other factors, such as the organization that developed the LLM, domain knowledge coverage, languages, and other features such as tool calling abilities. The best way to reduce costs is to find models that have exactly the features we need for data analysis, and do not have the features we do not need.</p>

<p>The features you surely need:</p>
<ul>
  <li>Good tool calling abilities</li>
  <li>Programming (specifically SQL for Zaturn)</li>
  <li>Thinking &amp; Reasoning.</li>
</ul>

<p>The features you might need:</p>
<ul>
  <li>Image input: If you want the AI to comment on generated images</li>
  <li>Multilingual support: If you or your users want to prompt in languages other than English</li>
</ul>

<p>The features you mostly don’t need:</p>
<ul>
  <li>Vast domain expertise</li>
</ul>

<p>In the following sections, let’s look at some of these factors and the most cost-effective models for each factor.</p>

<h2 id="programming-abilities">Programming Abilities</h2>

<p>Data analysis involves writing code to crunch numbers. Unless AI models are good at that, they will not be of much use. So it’s best to pick models that are good at writing code based on human instructions. With Zaturn, data analysis happens through SQL code, which hasn’t changed much in recent years as much as Python or NodeJS frameworks, syntax, and best practices keep changing. So you could cut some costs by choosing an older model, if that is an option.</p>

<p>The following are some models that work best for programming:</p>
<ul>
  <li>GLM 4.5 by z.ai</li>
  <li>Qwen3 Coder by Qwen</li>
  <li>Kimi K2 by MoonshotAI</li>
  <li>Gemini 2.5 Flash by Google</li>
</ul>

<p>Finally, some models that aren’t optimized for programming still work decently well with SQL, as SQL is not as complex as other languages. So the models listed above aren’t your only options.</p>

<h2 id="tool-use">Tool Use</h2>

<p>Zaturn or any other framework that helps LLMs analyze data is bound to work by providing tools to the LLM, which it can use to query the data it needs to query in order to reply to user prompts. So, tool-use ability is a make-or-break criterion while picking an LLM for data analysis. The following are some cost-effective AI models for which I’ve seen decent performance with Zaturn:</p>

<ul>
  <li>Gemini 2.5 Flash Lite by Google</li>
  <li>GLM 4.5 by z.ai</li>
  <li>Grok 3 Mini by xAI</li>
  <li>GPT-4.1 Nano by OpenAI</li>
</ul>

<h2 id="open-source--self-hostability">Open Source / Self-hostability</h2>

<p>The Zaturn project is open source and not a SaaS. One of the key advantages of this is that the tools can run locally or on-premises - your database URLs and files aren’t shared with anybody else. The only data that ever goes to a 3rd party is the conversations you have with the LLM, and the data pulled up during the tool usage in these conversations. If you self-host the LLM on your premises, then absolutely no data will be sent to any 3rd party - your full AI-based data analysis stack remains on premises.</p>

<p>Here are some self-hostable models that work best for data analysis:</p>
<ul>
  <li>GLM 4.5 by z.ai</li>
  <li>Qwen 3 32B by Qwen</li>
</ul>

<p>Some folks also self-host AI models with the intention of reducing the carbon footprint produced by power-hungry GPUs. While this works well for lightweight models that can run on commodity hardware CPUs, having a GPU at home does not necessarily make the approach any greener. In fact, data center GPUs typically are under centralized power use optimizations, while having a powerful GPU at home might actually be a misuse of electricity supplied for domestic purposes.</p>

<h2 id="context-window">Context Window</h2>

<p>Each LLM has a context window that ranges anywhere from a meagre 4K tokens to more than a million tokens. When the context window runs out, a new conversation has to be started, and the LLM will not have any context of the previous messages in the old conversations.</p>

<p>For data analysis tasks, LLMs with smaller context windows cannot do much more than write one or two SQL queries after being presented with a table structure and a prompt. The conversational/agentic experience is lost, and it’s only as good as an SQL autocomplete tool.</p>

<p>From my experience, I’d recommend a context window of at least 128K when you’re asking a specific question about your data. This will leave some room for follow-up questions too. Further, a context window of 1M tokens works amazingly well for in-depth exploratory data analysis. You can let an LLM loose on your data with a prompt like “Here’s all the data from our business, go through it in detail and give us a department-wise action plan for the next quarter.” The LLM can then run a handful of queries and summarize the results for you, keeping all the results in context together.</p>

<p>Here are some LLMs with high context windows (1M+ tokens) that work well for data analysis:</p>

<ul>
  <li>Gemini 1.5, 2, and 2.5 series by Google</li>
  <li>GPT-4.1 and variants by OpenAI</li>
  <li>Qwen Turbo by Qwen</li>
</ul>

<h2 id="thinking--reasoning">Thinking &amp; Reasoning</h2>

<p>Answering questions using data often involves planning, drafting a series of SQL queries, running them, getting results, and summarizing the results into a concise, useful insight for humans. Models with reasoning and thinking abilities are able to do this effortlessly with a separate budget for “reasoning tokens”.</p>

<p>Models that cannot do this have to be prompted for each step, like “Step 1: Get the orders from the Orders table grouped by Customer; Step 2: Join this with the Customer table and get an aggregate of orders grouped by Customer country.” A model with reasoning abilities can work with a simple prompt, “Which countries are generating the most revenue for us?”. It will automatically go through any table as necessary, perform joins, and produce the required results. If any step fails, the AI will automatically understand what went wrong and try alternative approaches. It can do a lot more with just one prompt.</p>

<h2 id="multimodality">Multimodality</h2>

<p>Data analysis isn’t just text - it involves a lot of visualizations that are images and sometimes even animated GIFs. Zaturn itself includes tools for generating visuals. LLMs operate with different input and output modalities, such as text, audio, image, and video.</p>

<p>For data analysis, only text as an output modality is sufficient for most use cases, as the visuals are generated by the tools and not by the AI model. For input modalities, text works for number crunching and generating verbal insights, while the image input modality is useful to help the AI see and comment on the visuals it generates through tool use. Audio as an input/output modality can help you “talk” to the data analysis agent and add to the coolness factor.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this blog, we went over the different factors to consider while picking an LLM to work with data as a data analysis agent. Tool-use capabilities, context-window size, and reasoning abilities are the crucial factors that can make or break your experiences. Most models that are good with tool use are fairly decent at writing SQL, so this factor is not much of a separate consideration.</p>

<p>Considering these factors and cost as well, Gemini 2.5 Flash Lite by Google would be my top recommendation. It costs $0.10 / 1M input tokens, $0.40 / 1M output tokens, and supports image inputs as well. The only downside is that it’s not self-hostable. Among the self-hostable models, my current recommendation would be GLM 4.5 by z.ai.</p>


		  
		</section>
	  </div>
    </main>
	<footer>
	  <div>
	    <div>© <a href="https://kdqed.com">Karthik Devan</a> (maintainer of Zaturn)</div>
	    <nav>
	    </nav>
	  </div>
	</footer>
  </body>
  
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <!-- 100% privacy-first analytics -->
  <script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>

</html>
